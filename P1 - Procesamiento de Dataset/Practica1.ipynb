{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1535e533",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "## Práctica 1\n",
    "Esta práctica ha sido llevada a cabo en solitario por Jaime Alonso Fernández (2024/2025) para la asignatura optativa \n",
    "\"Aprendizaje Automático y Big Data\" en el grado de \"Ingeniería del Software - plan 2019\" cursado en la \n",
    "Universidad Complutense de Madrid (UCM).\n",
    "\n",
    "\n",
    "## Importamos las librerías necesarias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a946c73f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from surprise import model_selection as ms, Dataset, Reader, prediction_algorithms as pa, accuracy as acu\n",
    "from sklearn import metrics as met"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93431eb2",
   "metadata": {},
   "source": [
    "# *APARTADO A*\n",
    "## Parte 1: Lectura de datos y adecuación al contexto\n",
    "Para esta primera parte, comenzamos usando la librería 'pandas' para leer y eliminar todos los archivos \n",
    "duplicados o con información vacía. Todo este proceso se lleva a cabo para trabajar solo con información nueva\n",
    "y precisa en el futuro."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f1efaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Importamos los datos del archivo\n",
    "titanic_data = pd.read_csv('titanic.csv')\n",
    "\n",
    "# Vamos a trabajar con otra variable cuando modifiquemos los datos\n",
    "# Eliminamos las filas duplicadas\n",
    "titanic_data_cleaned = titanic_data.drop_duplicates()\n",
    "# Eliminamos también aquellas filas con información vacía\n",
    "titanic_data_cleaned = titanic_data_cleaned.dropna()\n",
    "\n",
    "# Reiniciamos los índicies de esta información \"limpia\"\n",
    "titanic_data_cleaned = titanic_data_cleaned.reset_index(drop=True)\n",
    "\n",
    "# Si ahora comparamos las longitudes de titanic_data_cleaned y de titanic_data, sabremos el número de filas eliminadas\n",
    "print(len(titanic_data)-len(titanic_data_cleaned)) # $708"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fccd60cc",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "Como podemos ver en el resultado de la ejecución, se han eliminado un total de 708 filas. Cabe mencionar que para el \n",
    "caso específico de este dataset, no había ningún elemento duplicado.\n",
    "\n",
    "Ahora con esta información ya \"limpia\" podemos proceder a trabajar con los datos sin que salgan comparativas raras\n",
    "pues no todos los valores son reales y válidos.\n",
    "\n",
    "\n",
    "---\n",
    "## Parte 2: Atributos Redundantes\n",
    "Antes de pasar a comparar datos, es importante entender que podemos esperarnos de los mismos y tratar de hacer \n",
    "una computación eficiente con ellos. Para eso, tenemos que valorar que columnas pueden contener información \n",
    "relevante y sobre todo, cuáles no, de manera que podamos descartar estas últimas.\n",
    "\n",
    "En este caso en particular, yo he determinado que las siguientes columnas como redudantes:\n",
    "- PassengerId: No contiene información importante para el modelo.\n",
    "- Name: No considero relevante sacar la tasa de supervivencia de los \"Juanes\" o las \"Raqueles\", aunque socialmente pudiera estar ligado a la clase de los mismos.\n",
    "- Ticket: La información que pudiera ser relevante está asociada a la clase, las cuales van aparte.\n",
    "- Cabin: Entendiendo que la gente no tenía porque estar en su cabina en el momento del impacto, esta información es inútil.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb590d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardo los valores de los atributos redundantes en una lista para poder omitirlos\n",
    "atributos_redudantes = [\"PassengerId\", \"Name\", \"Ticket\", \"Cabin\"]\n",
    "# Eliminamos los atributos\n",
    "titanic_data_cleaned = titanic_data_cleaned.drop(atributos_redudantes, axis=1)\n",
    "# Y los mostramos en el notebook\n",
    "titanic_data_cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea865dd",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "\n",
    "---\n",
    "## Parte 7: Numerizando los atributos categóricos\n",
    "*DISCLAIMER: Esta celda requiere de la ejecución de la parte 1 para poder funcionar*\n",
    "\n",
    "Antes de nada aclarar que debido a la naturaleza del ejercicio 3, dónde se necesita calcular la matriz de correlación, \n",
    "he decidido adelantar este apartado ya que es necesario que todas las variables categóricas estén \"numerizadas\" para \n",
    "calcular su correlación. \n",
    "\n",
    "En este caso vamos a \"numerizar\" las variables mediante dos métodos diferentes: get_dummies y LabelEncoder; y \n",
    "compararlos para decidir cual considero mejor para un entrenamiento de IA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d74ab704",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos las variables categóricas:\n",
    "categorical = [\"Sex\", \"Embarked\", \"Survived\",\"Pclass\"]\n",
    "categorical_textual = [\"Sex\", \"Embarked\"]\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Declaramos una variable donde almacenar los resultados del LabelEncoder copiando si referencia \"titanic_data_cleaned\"\n",
    "titanic_label_encoded = titanic_data_cleaned.copy(deep=True)\n",
    "\n",
    "# Por cada columna categorica, la transformamos a numérica y la guardamos en la variable anterior\n",
    "for c in categorical:\n",
    "    titanic_label_encoded[c] = label_encoder.fit_transform(titanic_data_cleaned[c])\n",
    "\n",
    "# Utilizamos pandas para hacer un símil del OneHotEncoder\n",
    "titanic_dummified = pd.get_dummies(titanic_data_cleaned, categorical_textual)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae14206",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "Mostramos los datos en el notebook del OneHotEncoder o get_dummy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e40927",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "titanic_dummified"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfea451f",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "Mostramos los datos en el notebook del LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a72b2b",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "titanic_label_encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8e0a491",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "Finalmente podemos observar lo siguiente:\n",
    "\n",
    "Por una parte el OneHotEncoder crea una nueva columna booleana por cada elemento que conforma esa categoría,\n",
    "alargando la tabla y sobrando siempre una de las columnas (Puesto que se puede inferir al haber solo un \"True\" por fila).\n",
    "\n",
    "Por otra parte, el LabelEncoder decide asignar un valor entero para cada elemento que conforma esa categoría,\n",
    "manteniendo la longitud original de la tabla y sin columnas redundantes.\n",
    "\n",
    "En resumen y en lo personal, considero que es más limpia y correcta la opción que te ofrece el LabelEncoder, pues \n",
    "en términos de espacio es más eficiente y por ello es la opción que usaré de cara a esta práctica.\n",
    "\n",
    "\n",
    "---\n",
    "## Parte 3: Relaciones entre atributos\n",
    "*DISCLAIMER: Esta celda requiere de la ejecución de las partes 1 y 7 para poder funcionar*\n",
    "\n",
    "En este apartado vamos a analizar que variables comparten un vínculo más fuerte entre si y que variables darán \n",
    "son más solitarias. Esto permite entender la relación entre las diferentes variables, capacitandonos para entrenar \n",
    "un modelo con información más precisa. De todas formas esta información tambien puede contener sesgos que transciendan \n",
    "la vericidad de los datos por lo que habría que actuar de una manera cauta ante esta información. \n",
    "\n",
    "El rango de valores susceptibles a ser tomados es de [1, -1]. Los valores más próximos a 0 serán aquellos que menos tengan \n",
    "que ver mientras que los valores cuyo valor absoluto se aproxime más a 1 (los maś cercanos a los extremos), serán más similares."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de33853c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sacamos la matriz de correlación:\n",
    "titanic_corr = titanic_label_encoded.corr()\n",
    "titanic_corr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df49502",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "Mostramos la gráfica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e08a9079",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(titanic_label_encoded)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51c6a2f2",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "Como podemos observar, especialmente en la variable de correlación, la mayoría de las variables no se afectan en \n",
    "gran medida entre si. Sin embargo podemos destacar las siguientes que si lo hacen:\n",
    "\n",
    "Relaciones relevantes (>0,3)\n",
    "- Sexo y Supervivencia (0.532418): Atendiendo al protocolo tomado dónde las mujeres y los niños desembarcaron \n",
    "primero, tiene sentido que se de esta situacion. Quiero mencionar que en el DataSet se puede observar que no había \n",
    "una gran población infantil ni mayor, por lo que limita la capacidad de la relación de edad y supervivencia.\n",
    "\n",
    "- Tarifa y Congéneres (0.389740): Es posible que existieran ciertos descuentos a la hora de comprar los billetes \n",
    "en familia. De todas formas esto no se puede llegar a probar y la correlación no es tan alta como para afirmarlo \n",
    "rotundamente. Otra opción es que hubieran promociones o concursos realizados por terceros que dieran esta opción. \n",
    "De todas formas no se puede probar.\n",
    "\n",
    "- Clase y tarifa (0.315235): Las clases más bajas tendían a ser más baratas mientras que las más altas contaban \n",
    "con dos opciones: Que fueras un invitado, por lo que pagarías menos; o que pagases más que las clases más bajas. \n",
    "Personalmente sospecho que esto es lo que causa una correlación baja entre estos elementos, o que al menos esta\n",
    "correlación sea más baja de lo que cabría esperar.\n",
    "\n",
    "- Edad y PClass (0.306514): Es lógico que hasta cierto punto la edad determine el poder adquisitivo de cada uno \n",
    "y mientras que familias con infantes es más probable que optasen por un billete de clase inferior (y como los \n",
    "infantes contribuyen a las estadisticas, estas bajaran), gente que fuera sola o solo con su pareja se podrían \n",
    "permitir mejores billetes, y para ir solo has de tener cierta edad. \n",
    "\n",
    "\n",
    "---\n",
    "## Parte 4: Métricas de las variables numéricas\n",
    "*DISCLAIMER: Esta celda requiere de la ejecución de las partes 1 y 7 para poder funcionar*\n",
    "\n",
    "En este apartado procederemos a calcular las estadísticas de las variables numéricas de nuestro DataSet. Para ello \n",
    "haremos uso de la función *pandas.describe* la cual nos devuelve todas las estdísticas para cada una de las variables. Este \n",
    "resultado será mostrado en la pantalla y guardado en un diccionario que asocie cada resultado a su variable, permitiendo \n",
    "un fácil acceso a la información en caso de que se necesite a futuro."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c8d657",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Definimos las variables numéricas:\n",
    "numerical = [\"Age\", \"SibSp\", \"Parch\", \"Fare\"]\n",
    "# Definimos el diccionario para los resultados\n",
    "statistics = {}\n",
    "\n",
    "# Aplicamos el comando describe() de pandas sobre todos los valores numericos\n",
    "for variable in numerical:\n",
    "    print(f\"{variable}: \")\n",
    "    description = titanic_data_cleaned[variable].describe()\n",
    "    print(description)\n",
    "    print()\n",
    "    statistics[variable] = description"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf34143",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "\n",
    "---\n",
    "## Parte 5: Métricas de las variables categóricas\n",
    "*DISCLAIMER: Esta celda requiere de la ejecución de las partes 1, 7 y 4 para poder funcionar*\n",
    "\n",
    "En este apartado procederemos a calcular las estadísticas de las variables categoricas de nuestro DataSet. Para ello \n",
    "haremos uso de la función *pandas.describe* la cual nos devuelve todas las estdísticas para cada una de las variables. Este \n",
    "resultado será mostrado en la pantalla y guardado en el diccionario anterior que asocia cada resultado a su variable, permitiendo \n",
    "un fácil acceso a la información en caso de que se necesite a futuro."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e32970",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "\n",
    "# Aplicamos el comando describe() de pandas sobre todos los valores numericos\n",
    "for variable in categorical:\n",
    "    print(f\"{variable}: \")\n",
    "    description = titanic_data_cleaned[variable].astype('category').describe()\n",
    "    print(description)\n",
    "    print()\n",
    "    statistics[variable] = description\n",
    "    # Hacemos también una representación por cada una de las variables en formato histograma\n",
    "    plt.figure()\n",
    "    sns.histplot(titanic_data_cleaned[variable].astype('category'))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd21242f",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "\n",
    "---\n",
    "## Parte 6: Determinando outliers\n",
    "*DISCLAIMER: Esta celda requiere de la ejecución de las partes 1, 7 y 4 y 5 para poder funcionar*\n",
    "\n",
    "En este apartado vamos a trabajar en base a los datos obtenidos anteriormente para valorar la presencia de outliers \n",
    "en el DataSet en que nos encontramos. \n",
    "\n",
    "Para ello podemos utilizar dos maneras, una gráfica (empleando boxplot o scatterplot en función del tipo de variable) \n",
    "o mediante unas fórmulas ligadas al rango entre quartiles (IQR), la cual emplea el quartil 1 y 3 (percentiles 25 y 75).\n",
    "En esta iteración, voy a hacer una representación mediante boxplot ya que considero que se destacan más los outliers\n",
    "que en el scatterplot.\n",
    "\n",
    "Los valores de los quartiles los tenemos almacenados en el diccionario 'statistics', lo que nos provee de un fácil \n",
    "acceso a la inforamción. Destaquemos que no se hará el cálculo de las variables categóricas ya que la definición de \n",
    "outliers para estas variables no posee la misma relevancia (si poseé alguna) que las variables numéricas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fddf0bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for variable_name, variable_stat in statistics.items():\n",
    "    try:\n",
    "        # Probamos a sacar los quartiles 3 y 1 de las estadísticas. Saltará un error si las variables son categóricas\n",
    "        Q3, Q1 = variable_stat['75%'], variable_stat['25%']\n",
    "        # Cálculo del rango entre cuartiles\n",
    "        IQR =  Q3 - Q1\n",
    "        count = sum(1 for x in titanic_label_encoded[variable_name] if x >= 1.5*IQR+Q3 or x <= Q1-1.5*IQR)\n",
    "        # Hacemos el boxplot\n",
    "        plt.figure()\n",
    "        sns.boxplot(titanic_data_cleaned[variable_name])\n",
    "        plt.show()\n",
    "        # Mostramos los resultados numéricos\n",
    "        if count == 0:\n",
    "            print(f\"La variable {variable_name} no cuenta con outliers.\")\n",
    "        else:\n",
    "            print(f\"La variable {variable_name} cuenta con {count} outliers.\")\n",
    "    # No existe campo '75%' ni '25%' -> Variable categórica\n",
    "    except KeyError:\n",
    "        print(\"----\")\n",
    "        print(f\"Omitida variable {variable_name} por ser categórica.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd74cf3b",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "\n",
    "---\n",
    "## Parte 8: Normalizando y estandarizando el DataSet\n",
    "*DISCLAIMER: Esta celda requiere de la ejecución de las partes 1, 7 y 4 y 5 para poder funcionar*\n",
    "\n",
    "En este apartado vamos a hacer uso de las estadísticas del DataSet de pandas (mínimo, máximo, media, dev_est) para \n",
    "normalizar y estandarizar el DataSet.\n",
    "\n",
    "La normalización se hará mediante la fórmula de escalamiento de Min-Max. Esta dice así:\n",
    "X_normalizada = (X-X_min)/(X_max-X_min) \n",
    "\n",
    "\n",
    "La estandarización se hará mediante la fórmula de estandarización o Z-scoring. Esta dice así:\n",
    "X_estandarizada = (X-mean(X))/desv_est(X)\n",
    "\n",
    "Mencionar también que haremos uso del set tras haber aplicado el label encoder ya que necesitamos las variables numéricas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be4d0f15",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "\n",
    "t_min, t_max = titanic_label_encoded.min(), titanic_label_encoded.max()\n",
    "titanic_normalized = (titanic_label_encoded-t_min)/(t_max-t_min)\n",
    "sns.pairplot(titanic_normalized)\n",
    "plt.show()\n",
    "titanic_normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4476f6cd",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "\n",
    "t_mean, t_std = titanic_label_encoded.mean(), titanic_label_encoded.std()\n",
    "titanic_standarized = (titanic_label_encoded-t_mean)/(t_std)\n",
    "\n",
    "sns.pairplot(titanic_standarized)\n",
    "plt.show()\n",
    "titanic_standarized"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab552ef5",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "Como podemos observar en los gráficos, la estructura es idéntica en ambos casos (e igual que en la Parte 3). Esto es \n",
    "sucede debido a la naturaleza de la transformación lineal que realizamos. De todas formas los plots si que nos permiten ver\n",
    "cambios que no son perceptibles tan facilmente solo viendo los números: el rango.\n",
    "\n",
    "Este es al factor más determinante a la hora de elegir una fórmula para trabajar con los datos, detacando que sea como \n",
    "fuere la estructura inicial, la normalización siempre te deja el rango entre 0 y 1, lo cual facilita los cálculos estadísitcos.\n",
    "Por otra parte la estandarización escala los datos a un rango más pequeño pero variable con cada medida. Esto puede \n",
    "provocar problemas de inconsistencia de escalas a la hora de realizar los cálculos (obligando a que estos sean porcentuales)\n",
    "lo cual no es tan cómodo para trabajar con ello.\n",
    "\n",
    "Como conclusión, optaré por utilizar en la mayoría de los casos una estructura normalizada ante una estandarizada.\n",
    "\n",
    "\n",
    "# *APARTADO B*\n",
    "\n",
    "\n",
    "## Parte 1:\n",
    "\n",
    "Comenzamos cargando los datos usando pandas. De esta manera vamos a tener un DataFrame que contenga toda nuestra \n",
    "información siguiendo la estructura de columnas presente en el archivo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e2251abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Determinamos la seed \n",
    "SEED = 22082022\n",
    "\n",
    "# Cargar los datos\n",
    "column_names = ['user_id', 'item_id', 'rating', 'timestamp']\n",
    "ml100_dataframe = pd.read_csv('ml100.data', sep='\\t', names=column_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c53122d3",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "---\n",
    "## Parte 2:\n",
    "\n",
    "*DISCLAIMER: Esta celda requiere de la ejecución de las parte 1 para poder funcionar*\n",
    "\n",
    "En este apartado establecemos un lector que delimita los posibles valores de la variable rating de nuestros datos \n",
    "entre 1 y 5. Seguidamente convertimos el DataFrame de pandas a un Dataset de surprise, otorgando la posibilidad \n",
    "de trabajar con surprise sin problema. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "47c8a219",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Longitud datos: 100000\n",
      "Longitud set de entrenamiento: 75000\n",
      "Longitud set de evaluación: 25000\n"
     ]
    }
   ],
   "source": [
    "# Declaramos el reader\n",
    "reader = Reader(rating_scale=(1, 5))\n",
    "# Convertimos a Dataset omitiendo la última columna pues no será relevante para este apartado\n",
    "ml100_dataset = Dataset.load_from_df(ml100_dataframe[['user_id', 'item_id', 'rating']], reader)\n",
    "# Usamos la función division para entreno de surprise para dividir \"aleatoriamente\" (mediante SEED) el dataset en 75-25\n",
    "train_set, test_set = ms.train_test_split(ml100_dataset, test_size=0.25, random_state=SEED)\n",
    "\n",
    "# Verificamos que la división se haya realizado correctamente\n",
    "print(f\"Longitud datos: {len(ml100_dataframe)}\")\n",
    "print(f\"Longitud set de entrenamiento: {train_set.n_ratings}\")\n",
    "print(f\"Longitud set de evaluación: {len(test_set)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ea6436",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "---\n",
    "## Parte 3: \n",
    "\n",
    "*DISCLAIMER: Esta celda requiere de la ejecución de las partes 1 y 2 para poder funcionar*\n",
    "\n",
    "Para este apartado vamos a emplear diferentes algoritmos de recomendación y compararlos con la idea de elegir un \n",
    "mejor candidato para nuestro trabajo.\n",
    "\n",
    "Por una parte usaremos el Filtrado Colaborativo de Vecinos KNN de la biblioteca surprise usando la métrica de Pearson.\n",
    "\n",
    "Por otra parte emplearemos la factorización de matrices usando los algoritmos SVD (Single Value Decomposition) y \n",
    "NMF (non Negative Matrix Factorization). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "efe19d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Vamos a crear un diccionario para almacenar las opciones de simulación\n",
    "sim_options = {\n",
    "    \"name\": \"pearson\",\n",
    "    \"user_based\": True\n",
    "}\n",
    "# Comenzamos por KNN aplicado a los usuarios  \n",
    "knn_user_based = pa.knns.KNNBasic(sim_options=sim_options, random_state=SEED)\n",
    "\n",
    "# Reconfiguramos el diccionario\n",
    "sim_options[\"user_based\"] = False\n",
    "# Hacemos KNN aplicado a productos\n",
    "knn_product_based = pa.knns.KNNBasic(sim_options=sim_options, random_state=SEED)\n",
    "\n",
    "# Pasamos a hacer la sección de SVD\n",
    "SVD_set = pa.matrix_factorization.SVD(random_state=SEED)\n",
    "\n",
    "# Y finalmente NMF\n",
    "NMF_set = pa.matrix_factorization.NMF(random_state=SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07667d71",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "---\n",
    "## Parte 4: \n",
    "\n",
    "*DISCLAIMER: Esta celda requiere de la ejecución de las partes 1, 2 y 3 para poder funcionar*\n",
    "\n",
    "Ya preparado el espacio de trabajo, hay que entrenar los diferentes modelos con los datos de entreno (train_data)\n",
    "\n",
    "Para esto usaremos el método fit que tienen todos los modelos de la librería surprise que estamos utilizando"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cebfc835",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing the pearson similarity matrix...\n",
      "Done computing similarity matrix.\n",
      "Computing the pearson similarity matrix...\n",
      "Done computing similarity matrix.\n"
     ]
    }
   ],
   "source": [
    "# Entrenamos los KNN\n",
    "knn_user_trained = knn_user_based.fit(trainset=train_set)\n",
    "knn_product_trained = knn_product_based.fit(trainset=train_set)\n",
    "\n",
    "# Entrenamos los algoritmos de factorización de matrices\n",
    "SVD_trained = SVD_set.fit(trainset=train_set)\n",
    "NMF_trained = NMF_set.fit(trainset=train_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56fd0141",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "---\n",
    "## Parte 5:\n",
    "\n",
    "*DISCLAIMER: Esta celda requiere de la ejecución de las partes 1, 2, 3 y 4 para poder funcionar* \n",
    "\n",
    "Con los modelos entrenados, pasaremos a probar que funciones bien mendiante la opción test con los datos de set (test_set).\n",
    "\n",
    "Finalmente tomaremos las 5 primeras predicciones y discutiremos los resultados. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b679becb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elemento 0:\n",
      "\tPara knn_user:\n",
      "\tuser: 650        item: 434        r_ui = 4.00   est = 3.00   {'actual_k': 40, 'was_impossible': False}\n",
      "\tPara knn_product:\n",
      "\tuser: 650        item: 434        r_ui = 4.00   est = 3.00   {'actual_k': 40, 'was_impossible': False}\n",
      "\tPara SVD:\n",
      "\tuser: 650        item: 434        r_ui = 4.00   est = 3.49   {'was_impossible': False}\n",
      "\tPara NMF:\n",
      "\tuser: 650        item: 434        r_ui = 4.00   est = 3.52   {'was_impossible': False}\n",
      "Elemento 1:\n",
      "\tPara knn_user:\n",
      "\tuser: 200        item: 118        r_ui = 4.00   est = 3.76   {'actual_k': 40, 'was_impossible': False}\n",
      "\tPara knn_product:\n",
      "\tuser: 200        item: 118        r_ui = 4.00   est = 3.76   {'actual_k': 40, 'was_impossible': False}\n",
      "\tPara SVD:\n",
      "\tuser: 200        item: 118        r_ui = 4.00   est = 4.02   {'was_impossible': False}\n",
      "\tPara NMF:\n",
      "\tuser: 200        item: 118        r_ui = 4.00   est = 3.66   {'was_impossible': False}\n",
      "Elemento 2:\n",
      "\tPara knn_user:\n",
      "\tuser: 870        item: 603        r_ui = 5.00   est = 3.32   {'actual_k': 40, 'was_impossible': False}\n",
      "\tPara knn_product:\n",
      "\tuser: 870        item: 603        r_ui = 5.00   est = 3.32   {'actual_k': 40, 'was_impossible': False}\n",
      "\tPara SVD:\n",
      "\tuser: 870        item: 603        r_ui = 5.00   est = 4.42   {'was_impossible': False}\n",
      "\tPara NMF:\n",
      "\tuser: 870        item: 603        r_ui = 5.00   est = 4.31   {'was_impossible': False}\n",
      "Elemento 3:\n",
      "\tPara knn_user:\n",
      "\tuser: 447        item: 1048       r_ui = 2.00   est = 3.71   {'actual_k': 40, 'was_impossible': False}\n",
      "\tPara knn_product:\n",
      "\tuser: 447        item: 1048       r_ui = 2.00   est = 3.71   {'actual_k': 40, 'was_impossible': False}\n",
      "\tPara SVD:\n",
      "\tuser: 447        item: 1048       r_ui = 2.00   est = 3.01   {'was_impossible': False}\n",
      "\tPara NMF:\n",
      "\tuser: 447        item: 1048       r_ui = 2.00   est = 2.97   {'was_impossible': False}\n",
      "Elemento 4:\n",
      "\tPara knn_user:\n",
      "\tuser: 16         item: 209        r_ui = 5.00   est = 4.19   {'actual_k': 40, 'was_impossible': False}\n",
      "\tPara knn_product:\n",
      "\tuser: 16         item: 209        r_ui = 5.00   est = 4.19   {'actual_k': 40, 'was_impossible': False}\n",
      "\tPara SVD:\n",
      "\tuser: 16         item: 209        r_ui = 5.00   est = 4.35   {'was_impossible': False}\n",
      "\tPara NMF:\n",
      "\tuser: 16         item: 209        r_ui = 5.00   est = 4.31   {'was_impossible': False}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Comenzamos realizando los tests:\n",
    "resultados_test = {}\n",
    "# Ejecutamos los test y los \n",
    "resultados_test['knn_user'] = knn_user_trained.test(test_set)\n",
    "resultados_test['knn_product'] = knn_product_trained.test(test_set)\n",
    "resultados_test['SVD'] = SVD_trained.test(test_set)\n",
    "resultados_test['NMF'] = NMF_trained.test(test_set)\n",
    "\n",
    "# Mostramos los resultados de las predicciones\n",
    "for i in range(5):\n",
    "    print(f\"Elemento {i}:\")\n",
    "    for key, value in resultados_test.items():\n",
    "        print(f\"\\tPara {key}:\")\n",
    "        print(f\"\\t{value[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fed5b3b",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "\n",
    "Los resultados se muestran de la siguiente forma (acorde a las columnas):\n",
    "- ID usuario\n",
    "- ID película\n",
    "- Nota real\n",
    "- Nota predicha\n",
    "- Detalles (sin importancia)\n",
    "\n",
    "---\n",
    "## Parte 6: \n",
    "*DISCLAIMER: Esta celda requiere de la ejecución de las partes 1, 2, 3, 4 y 5 para poder funcionar*\n",
    "\n",
    "En este apartado usaremos 4 métricas de comparación para finalmente determinar que modelo es más preciso para \n",
    "nuestra valoracion. Estás métricas serán las siguientes:\n",
    "- RMSE -> Cuánto menos mejor\n",
    "- precision -> Cuánto más mejor\n",
    "- recall -> Cuánto más mejor\n",
    "- NDCG (solo para 10 elementos) -> Cuánto más mejor\n",
    "\n",
    "Para todos los algortimos salvo el RMSE, necesitaremos usar dos listas. Una de las listas contendrá todos los \n",
    "elementos con ranking real >= 4 y la otra, todos los elementos con ranking estimado >=4. Para el caso del RMSE vamos \n",
    "a coger solo los valores que cumplan ambas propiedades a la vez, pues este solo recibe una lista. \n",
    "\n",
    "Cabe mencionar que a la hora de aplicar del filtro para los valores >= 4, también vamos a ordenar la lista de manera \n",
    "que aquellos con la mejor nota estimada estén arriba del todo. Esto se hace para que cuando apliquemos el NDCG sobre\n",
    "10 elementos, estos elementos sean aquellos con la mejor nota estimada.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1f6fb81a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|-RMSE (<)-------------------------------------------------\n",
      "\tknn_user - 0.608307163375933\n",
      "\tknn_product - 0.608307163375933\n",
      "\tSVD - 0.5541695107982583\n",
      "\tNMF - 0.5516285963948792\n",
      "\n",
      "|-Precision, Recall y NDCG (>)-----------------------------\n",
      "\tknn_user:\n",
      "\t\tprecision: 0.7962507682851875\n",
      "\t\trecall: 0.18561501540224945\n",
      "\t\tNDCG: 0.7962507682851875\n",
      "\tknn_product:\n",
      "\t\tprecision: 0.7962507682851875\n",
      "\t\trecall: 0.18561501540224945\n",
      "\t\tNDCG: 0.7962507682851875\n",
      "\tSVD:\n",
      "\t\tprecision: 0.8499005964214712\n",
      "\t\trecall: 0.36750483558994196\n",
      "\t\tNDCG: 0.8499005964214712\n",
      "\tNMF:\n",
      "\t\tprecision: 0.8292201382033564\n",
      "\t\trecall: 0.3610573823339781\n",
      "\t\tNDCG: 0.8292201382033564\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Comenzamos seleccionando los valores que tengan un ranking >= 4 y aislandolos en un diccionario.\n",
    "resultados_filtrados = {}\n",
    "\n",
    "# El diccionario guardará por cada key una tupla (relevantes reales, relevantes estimados)\n",
    "# de manera que ambas listas de la tupla esten ordenadas de manera descendente en base al mismo criterio, x.est.\n",
    "for key, resultado in resultados_test.items():\n",
    "    # Ordenamos en base a r_est\n",
    "    resultado.sort(key=(lambda x: x.r_ui), reverse=True)\n",
    "    # Guardamos la tupla en el diccionario\n",
    "    resultados_filtrados[key] = ([int(result.r_ui >= 4) for result in resultado], \n",
    "                                 [int(result.est >= 4) for result in resultado])\n",
    "\n",
    "print(\"|-RMSE (<)-------------------------------------------------\")\n",
    "# Aplicamos el RMSE sobre todos los elementos\n",
    "for key, resultado in resultados_test.items():\n",
    "    # Filtramos los resultados de manera que solo tomemos aquellos con ambos valores >= 4\n",
    "    resultado = [result for result in resultado if result.r_ui >= 4 and result.est >= 4]\n",
    "    print(f\"\\t{key} - {acu.rmse(resultado, verbose=False)}\")\n",
    "\n",
    "print(\"\")\n",
    "print(\"|-Precision, Recall y NDCG (>)-----------------------------\")   \n",
    "# Aplicamos el resto de los algoritmos sobre los elmentos filtrados y ordenados\n",
    "for key, resultado in resultados_filtrados.items():\n",
    "    print(f\"\\t{key}:\")\n",
    "    print(f\"\\t\\tprecision: {met.precision_score(resultado[0], resultado[1])}\")\n",
    "    print(f\"\\t\\trecall: {met.recall_score(resultado[0], resultado[1])}\")\n",
    "    print(f\"\\t\\tNDCG: {met.ndcg_score([resultado[0]], [resultado[1]], k=10)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c08548",
   "metadata": {},
   "source": [
    "---\n",
    "## Parte 7: Conclusiones\n",
    "\n",
    "Analizando os datos podemos observar las siguientes tendencias:\n",
    "1. SVD y NMF superan a los métodos KNN en todas las métricas:\n",
    "\n",
    "  - - Hay que tener en cuenta que el RMSE es el único que cuanto menos mejor, por ello, podemos ver que en \n",
    "  todas las categorías, estos dos algortimos destacan.\n",
    "\n",
    "2. Igualdad en los KNN: \n",
    "\n",
    "  - - Para todas las métricas, el valor obtenido para el KNN de usuario iguala aquel obtenido para el KNN de producto. \n",
    "  Estas similitudes señalan que pueda haber una relación muy fuerte entre productos y usuarios. \n",
    "\n",
    "3. SVD es el mejor algortimo para nuestro caso:\n",
    "\n",
    "  - - Y aunque dijimos antes que el NMF y el SVD están muy a la par, es el SVD el que gana en todas las métricas. \n",
    "      De todas formas, ninguno ha tenido un RMSE lo suficientemente bajo como para determinar que es universalmente \n",
    "      bueno."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
