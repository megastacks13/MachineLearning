{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Práctica 3 - Deep Learning: Perceptrón Multicapa\n",
    "\n",
    "Durante este primera apartado, vamos a empezar con un algoritmo generador de datos como aquel visto en la P-2. Este algoritmo será el encargo de crear sintéticamente la sample con la que vamos a entrenar a nuestro modelo de IA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importamos las librerías\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos las constantes\n",
    "# -- Arterial\n",
    "MIN_ARTERIAL = 70\n",
    "NORMAL_ARTERIAL_LOW = 100\n",
    "NORMAL_ARTERIAL_HIGH = 120\n",
    "MAX_ARTERIAL = 180\n",
    "# -- Colesterol\n",
    "MIN_COLESTEROL = 120\n",
    "NORMAL_COLESTEROL_LOW = 120\n",
    "NORMAL_COLESTEROL_HIGH = 180\n",
    "MAX_COLESTEROL = 250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Método general de generación del dataset\n",
    "def generate_people(n: int) -> np.array:\n",
    "    # Iniciamos el dataset como una lista\n",
    "    dataset = []\n",
    "    for _ in range(n):\n",
    "        # Creamos un dataelement como una lista\n",
    "        dataelement = []\n",
    "        # Presión arterial \n",
    "        dataelement.append(np.random.randint(MIN_ARTERIAL, MAX_ARTERIAL + 1))\n",
    "        # Colesterol \n",
    "        dataelement.append(np.random.randint(MIN_COLESTEROL, MAX_COLESTEROL + 1))\n",
    "        # Añadimos el dataelement al dataset\n",
    "        dataset.append(dataelement)\n",
    "    # Transformamos el dataset a un array de numpy\n",
    "    dataset = np.array(dataset)\n",
    "    return dataset\n",
    "    \n",
    "\n",
    "# Calculamos las probabilidades\n",
    "def compute_probability_illness(dataset:np.array) -> np.array:\n",
    "    arterial, colesterol = dataset[:, 0], dataset[:, 1]\n",
    "        \n",
    "    prob_arterial = np.where(arterial < NORMAL_ARTERIAL_LOW,  # Si la presión arterial es menor a la normal menor\n",
    "                            (NORMAL_ARTERIAL_LOW - arterial) / (NORMAL_ARTERIAL_LOW - MIN_ARTERIAL), # El riesgo es el índice de desviación\n",
    "                            np.where(arterial > NORMAL_ARTERIAL_HIGH, # Si es mayor a la normal mayor\n",
    "                                        (arterial - NORMAL_ARTERIAL_HIGH) / (MAX_ARTERIAL - NORMAL_ARTERIAL_HIGH), # El riesgo es el índice de desviación\n",
    "                                        0)) # Si no es ni menor a la normal menor ni mayor a normal mayor, el riesgo es 0\n",
    "\n",
    "    prob_colesterol = np.where(colesterol > NORMAL_COLESTEROL_HIGH, # Si el colesterol es mayor al máximo normal\n",
    "                                (colesterol - NORMAL_COLESTEROL_HIGH) / (MAX_COLESTEROL - NORMAL_COLESTEROL_HIGH), # El riesgo es el índice de desviación\n",
    "                                0) # Si no el riesgo es 0\n",
    "\n",
    "\n",
    "    # Todas las probabilidades ostentan un mismo peso que suman como máximo 1\n",
    "    total_prob = 1/2 * prob_colesterol + 1/2 * prob_arterial\n",
    "\n",
    "    # Devolvemos el dataset añadiendole una nueva columna\n",
    "    return np.column_stack((dataset, total_prob))\n",
    "\n",
    "\n",
    "# Funcion que aplica cota sobre datos\n",
    "def apply_threshold(threshold:float, dataset:np.array) -> np.array:\n",
    "    risk_class = np.where(dataset[:, -1] > threshold, 1, 0)\n",
    "    return np.column_stack((dataset, risk_class))\n",
    "\n",
    "def get_dataset(n:int) -> np.array:\n",
    "    people = generate_people(n) # Generamos un dataset\n",
    "    people = apply_threshold(0.5, compute_probability_illness(people)) # Le añadimos las columnas de probabilidad en formato binario\n",
    "    return people"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generamos\n",
    "sample_people = get_dataset(1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "**Jaime Alonso Fernández**  \n",
    "**Universidad Complutense de Madrid**  \n",
    "**Grado en Ingeniería del Software (Plan 2019)**  \n",
    "**Asignatura de Aprendizaje Automático y Big Data**\n",
    "***\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
